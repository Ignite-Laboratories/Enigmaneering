# `E1.0 - Quarter Splitting`
## `A.K.A. - "Middle Out Compression"`
### `Alex Petz, Ignite Laboratories, June 2025`

---

### What if you read binary from the top?
Every great project starts with a simple question.  Let's explore this one a bit!

Binary gives us a few unique qualities that most numbers _don't._  For instance, the bit width (which we'll 
call ùëõ) directly defines the absolute largest value that number could _not_ be: 2‚Åø. It also gives us the 
smallest value it _could_ be: 2‚Åø-¬π.

The value exists _between_ those boundaries.

Let's call 2‚Åø-¬π the _light_ value, and (2‚Åø-1) the _dark_ value.

       2‚Åø [ 1 0 0 0 0 0 0 0 0 0 ] (512) <- Upper
     2‚Åø-1   [ 1 1 1 1 1 1 1 1 1 ] (511) <- Dark
            [ 1 1 0 1 0 0 1 0 1 ] (421) <- Target
            [ 1 1 0 0 0 0 0 0 0 ] (384) <- Mid
     2‚Åø-¬π   [ 1 0 0 0 0 0 0 0 0 ] (256) <- Light (or Lower)

The target _clearly_ exists only 37 above the mid point, but we still _store_ it as 421 above _0!_

Why!?

Well, '37' doesn't tell us anything meaningful unless we know _where_ to apply it to.  But we do!
This exists 37 steps _above_ the mid point between the powers of two that bound the target in - and
there are only _four_ different directions we could even walk:

    Up from the lower bound
    Down from the mid point
    Up from the mid point
    Down from the upper bound

Each of those directions represents a _quarter_ of the address space, which is coincidentally _two bits_
of information.  This allows us to start building an encoding scheme:

                   ‚¨ê The remainder bits
    [ ‚Å∞‚ÅÑ‚ÇÅ ‚Å∞‚ÅÑ‚ÇÅ ] [ ‚Å∞‚ÅÑ‚ÇÅ ... ]
         ‚¨ë The focus crumb key
    
    Key | Meaning
     00 | The remainder is read as up from the lower bound
     01 | The remainder is read as down from the mid-point
     10 | The remainder is read as up from the mid-point
     11 | The remainder is read as down from the upper bound

So, let's go back to our previous example and write out the encoded value of 421:

    [ 1 1 0 1 0 0 1 0 1 ] <- Target
    [ 1 0 - 1 0 0 1 0 1 ] <- Encoded

Immediately, we have gained a _single bit_ of reduction!  This is _fantastic_ - but don't start quarter splitting
every byte you find quite yet: _the length has changed_.  Your next measurement would not be readable because you
wouldn't know _when_ it starts!  However, this particular design has an exploit: it works better _at scale_ =)

In fact, this works for literally _any length_ of binary information!  Let's break down what a byte would look like:

    Key | Value Range
     00 | 0-63 (starting from 0)
     01 | 64-127 (starting from 127)
     10 | 128-191 (starting from 128)
     11 | 192-255 (starting from 255)

    [ 0 0 0 0 0 0 0 0 ]   (0) -> [ 0 0 ] (2 bits)
    [ 0 0 0 0 0 1 0 0 ]   (8) -> [ 0 0 - 1 0 0] (5 bits) 
    [ 0 0 1 0 1 0 1 0 ]  (77) -> [ 0 1 - 1 1 0 0 1 1] (8 bits)
    [ 0 1 1 1 1 1 1 1 ] (127) -> [ 0 1 - 1 ] (3 bits)
    [ 1 0 0 0 0 0 0 1 ] (129) -> [ 1 0 - 1 ] (3 bits)
    [ 1 1 0 1 1 1 1 0 ] (222) -> [ 1 1 - 1 0 0 0 1 0 ] (8 bits)
    [ 1 1 1 1 1 1 1 1 ] (255) -> [ 1 1 ] (2 bits)

Lets even break down a _note_ (3 bits):

    Key | Value Range
    00 | 0-1 (starting from 0)
    01 | 2-3 (starting from 3)
    10 | 4-5 (starting from 4)
    11 | 6-7 (starting from 7)

    [ 0 0 0 ] (0) -> [ 0 0 ]
    [ 0 1 0 ] (1) -> [ 0 0 - 1]
    [ 0 0 1 ] (2) -> [ 0 1 ]
    [ 0 1 1 ] (3) -> [ 0 1 - 1 ]
    [ 1 0 0 ] (4) -> [ 1 0 ]
    [ 1 0 1 ] (5) -> [ 1 0 - 1 ]
    [ 1 1 0 ] (6) -> [ 1 1 ]
    [ 1 1 1 ] (7) -> [ 1 1 - 1 ]

The first thing you'll notice is that key `01` starts at -1 from the mid point, which allows full coverage
of the range while including '0' as an addressable value.

### Recursion
As I said before, this applies to binary information _at any scale._  However, at the astronomical scales that
a _file_ exists at - the amount of bits that reduces from each operation is _significantly_ more beneficial.
To take advantage of this paradigm, however, we get to turn to my favorite topic: _recursion!_

I know, I know, it's the bane of many a programmer's existence - but it truly is beauty in motion, when
built well. Some of the best examples of computational efficiency evolved from recursive algorithms, and
I absolutely adore the amount of ingenuity it took for those enigmaneers to craft them.

So, let's now look at the _downside_ of binary information:

Logical binary information allows leading 0s

Numerical binary information does _not_

Let me explain - the first _byte_ of a file might start with several zeros, which is perfectly reasonable
in that context.  However, when the value converts to a _numeric_ form those zeros are simply _lost!_ This
will need more addressing, later on, but it's important to _consider_ right now.  To get around this, we
will do several very important things.

- The key information will always be on the _left_
- Each transformation round will be considered a 'movement' of binary information
- The very first bit of any movement is the 'terminus' bit, which always holds a value of '1'
- A 'recycle' bit in the key should indicate if the process synthesis should continue after this round completes
- The process of shrinking binary information is 'distilling'
- The process of growing binary information is 'synthesizing'

This means our abstract movement structure *currently* looks like this:

    Crude Abstract Movement Encoding:

            ‚¨ê Recycle Bit    ‚¨ê Value               
     [ 1 - ‚Å∞‚ÅÑ‚ÇÅ - ‚Å∞‚ÅÑ‚ÇÅ ‚Å∞‚ÅÑ‚ÇÅ ] [ ‚Å∞‚ÅÑ‚ÇÅ ... ] 
       ‚¨ë Terminus   ‚¨ë Focus Crumb

The first phrase is the _key,_ while the second is the _value_ and holds the actual data to encode.

Now, the one major point you've probably got in your mind is this: by prepending the data with bits, it will
sometimes grow _larger_ than the target bit width! And you'd be right!  However, it would only happen
when the value to encode is just the right distance from the boundary point - which diminishes in likelihood significantly
_at scale._  That being said, even if it _did_ grow during the distillation process it would then move it into a
new value position that is not sympathetic to the failure point - thus, it wouldn't _loop_ and you'd likely never
even notice when it occurs.

_Unless it would?_

Well, theoretically it _could_ reach one value and then distill down to a prior visited value, causing a loop.
But there's a simple solution to that concern called the _entropy bit,_ which I'll touch on later.

The recycle bit would be set to 0 during the first movement, and then 1 on any following movement.  When synthesizing
the value, this would signal that the final transformation should _not_ be read as an encoded value.  Instead it
represents the final form the data should be placed within.

The _only_ part this scheme does not yet encode is the amount of zeros to prepend to the data on each transformation,
but there is another solution for that which I call _Zero Length Encoding._ 
It's named as such because you count and read _zeros_ until you reach a _one,_ at which point you make a decision.
This allows a variably short-to-long set of bits that can describe small to large numbers with minimal extra bits.

ZLE comes in many different flavors, and the engineer would have to articulate what the intent is first.
The ability to articulate the growth scheme information will be touched on when we reach the composition phase.
For now, we'll talk about the two standard ZLE flavors: bounded and unbounded ZLE.

    Step 0 -
        Unbounded ZLE - Count the number of zeros read before a one is reached
        Bounded ZLE - Count up to a fixed number of zeros or a one is reached

    Step 1 -
        Parse the read number into a known value according to a key map - this is the Projection Value

    Step 2 -
        Read the defined number of bits according to the rules of your key map

I recognize at this point I haven't formally defined what a 'Read' operation even is, as it's a little different
from your typical 'read' operation:  it means you _advance_ through the bits by consuming the prescribed amount.
Many different structures support _Read_ operations, allowing you to "bleed off" a few bits at a time efficiently
in order to make logical decisions.

### Reconstitution
The most important aspect of binary synthesis is the _ability_ to reconstitute the original information.
As of this moment, this scheme has one crucially fatal flaw: we no longer know where the original bounds were!

Luckily, there's a solution for that - using ZLE we can store the amount of zeros that the data _reduced by._
This particular value is the _Delta_ and will typically be represented with the greek Œî symbol.
At this point, the below scheme represents the most _primitive_ encoding scheme for a movement possible - at
least, with _my_ currently understanding of this technology: 

    Primitive Movement Encoding:

            ‚¨ê Recycle Bit    ‚¨ê Œî ZLE          Value ‚¨é 
     [ 1 - ‚Å∞‚ÅÑ‚ÇÅ - ‚Å∞‚ÅÑ‚ÇÅ ‚Å∞‚ÅÑ‚ÇÅ ] [ ‚Å∞‚ÅÑ‚ÇÅ ... ] [ ‚Å∞‚ÅÑ‚ÇÅ ... ] [ ‚Å∞‚ÅÑ‚ÇÅ ... ] 
       ‚¨ë Terminus   ‚¨ë Focus Crumb        ‚¨ë Entropy Bit

But that leaves the most critial piece of this puzzle: how do I allow others to evolve this technology on their
own terms?
That will get tackled in more detail as we touch on _compositions_ - but for now, we will call the above the
'standard growth scheme' for binary information.
New growth schemes might completely replace what a movement event looks like, but the information would only
be visible at the _initializiation_ phase of synthesis - which we are still working our way towards, in reverse!

Now, the million dollar question that remains: _when do you stop distilling?_

That's easy: when you can no longer shrink the data by _prepending_ the encoding scheme data.  Every single
movement requires _adding_ data to the current bits, which means eventually the process would not allow your
data to shrink any further.  When I hit that boundary, I'll add further to this =)