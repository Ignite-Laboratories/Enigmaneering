# `E4S0.0 - Channel Loops`
### `Alex Petz, Ignite Laboratories, April 2025`

---

### Single Threaded Activation
To accomplish the kind of activation I desired, I had to make a fundamental change to the basic
_loop driver_.  Rather than firing off a _new_ Go routine for every activation, the neuron
itself sparks off a single Go routine that waits for a message on a channel to activate.  The
impulse engine now sends the context over that channel whenever the potential returns true, 
allowing a "single-threaded" looping activation.

This was the old code to create a looping neuron.  As you can see, it simply launches
a Go routine as its activation -

    func (e *Engine) Loop(action Action, potential Potential, muted bool) *Neuron {
        var n Neuron
        n.ID = NextID()
        n.engine = e
        n.Action = func(ctx Context) {
            n.executing = true
            go func() {
                action(ctx)
			    n.ActivationCount++
                n.executing = false
            }()
        }
        n.Potential = potential
        n.Muted = muted
        e.addNeuron(&n)
        return &n
    }

In the new design we use a channel and a single Go routine to signal between the impulse
and neural execution, minimizing the number of created Go routines - 

    func (e *Engine) LoopOnChannel(action Action, potential Potential, muted bool) *Neuron {
        var n Neuron
        n.ID = NextID()
        n.engine = e
        channel := make(chan Context)
        n.Action = func(ctx Context) {
            n.executing = true
            channel <- ctx
        }
        n.Potential = potential
        n.Muted = muted
    
        go func() {
            for ctx := range channel {
                if n.Destroyed {
                    // Close the channel goroutine
                    return
                }
    
                action(ctx)
			    n.ActivationCount++
                n.executing = false
            }
        }()
    
        e.addNeuron(&n)
        return &n
    }

### Implications
Now as this currently sits it isn't any more performant than the old iteration - in fact, it actually
has a _negative_ implication.  With the old scheme Go naturally balanced neural activation across
all the CPU cores, but with the new scheme it isn't as likely to "jump" between cores.  Because of this,
I've kept the channel loop as a separate function from the traditional loop.

I also did so because to really see the performance benefits side-by-side you _need_ a more taxing load.
Right now, especially at full tilt, we have no way to _spread_ such a single-threaded activation across
time - at least, not in an observably useful fashion.  Don't worry, that will make more sense as we
continue into the process of cascading activation.  For now we're gonna need to make some changes to the 
above design before I consider it to be "useful" - let's continue =)