# `E2 - Fuzzy Pattern Matching`
### `Alex Petz, Ignite Laboratories, March 2025`

---

### Neural Filters

The initial reason I started on this project was to pursue a new form of compression - one far more akin to 'synthesis'
than 'compression'.  The idea is simple - could I identifying the values necessary to compress progressively more complex 
data, and then turn around and literally recreate them?  If so, could that eventually evolve to _predict_ future actions 
from learned _instincts?_

What does it even _mean_ to "predict" something?

At its core a prediction is just a gut-reaction of what _might_ unfold, based on _experience_.  So, my perspective shift
was to consider that the _data itself_ is _experience_.  What does it even mean to _experience_ something?

Truly - _have you ever kissed a rose?_

What a silly expression - of course one can kiss a rose - yet Seal took it to stardom through _relatability_.  How can
one describe such an ineffable _feeling?_  It might land with one person or be an entirely foreign language for another!

Yet, cohesively, we all somehow connect on a _fuzzy_ and _abstract_ feeling.  Because we don't each need to know the
_exact_ feelings others experience - rather, we should feel _safe_ coexisting with _their_ interpretation of the current
environment!

The best way to describe this is to literally connect that we are all nodes in a gigantic neural network orchestrated
by the system that spawned us off.  Each of us are seeded with a unique signal from which to compare to the current
world around us.  When we identify our own signal, we fully activate - but we can also activate if the signal is _close
enough_.

The system observing all of the signals can identify when _two or more_ nodes cross over a specified threshold, and then
spawn off a replication of that to identify the more complex pattern _quicker_.  The spawned nodes look at the _exact same
data_ and represent the next layer of neural activation - a process repeated until the overall window of observance
required to uniquely identify patterns grows wider and wider.

At _this_ point the neurons can _instinctively_ go 'high' when a known condition is identified using a threshold approximation!

Those very instincts form the most _primitive puzzle pieces_ of synthesis =)

Once the window gets too wide to walk the identified patterns and provide fuzzy matches quickly, it would move to using
a 'key' lookup that filters _its_ list of identifiable waveforms by only processing the entries that match the start of the
data.

The _measurement_ window is the maximum width for a neural network to continue spawning activations from, beyond which it would
move to _phrase_ activation that searches off of the key data.

### Storage

There is no storage!  Once the system is online the input waveforms necessary to spawn off activations would be required to
recreate the system, but all it does it _fuzzily match waveforms!_  Four unique sources of waveforms to activate the neural
network should probably be enough to actively 'clean' any input data to a less energetic form.  The less noisy the data,
the easier it is to compress!  Any compression algorithm that can shrink the data length after a round of _neural filtration_ 
can be reused cyclically to shrink the data to its shortest form using the same de-noising system -> infinite compression.

The neuron becomes a source of identified primitive waveforms from which to make fuzzy approximations, but it doesn't
require all of the primitive activations that were necessary to create it any further - it's self-identified its own values.
Now, it can be a single activation in the neural network and get put through the same process again.  This recursively creates
more complex neurons to replace the last, each of which executes for longer and longer periods of time.

    tl;dr - the short neural activations collapse into a single longer activation with each round.