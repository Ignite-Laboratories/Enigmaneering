# Stack Learning
### A.K.A. Reverse Polish Notation Learning
#### Alex Petz, Ignite Laboratories, March 2025

---

### _How_ does an algorithm learn?
_What_ even is learning?

_What_ is learned?

_Learning._

Learning _is_ what is learned.

Learning what it even _is_ to learn.

### That's _how_ an algorithm learns

    tl;dr - learnA, learnB, calculate

_Oh, the recursion!_

## Stacks on stacks on stacks
This is best described through _Reverse Polish Notation_ - in which the operands are _queued up_ for an operator
to consume.  The starting conditions you provide a system with before activating a particular learning pathway 
entirely dictate the results the system will yield _while_ learning.  The system has free will throughout the 
entire process to choose its desired path using the provided contextual "signposts".  Ultimately, no matter
what pathway the system chooses to reach the end goal - _maturity_ - it uses the _same_ toolkit at every step.

    (operandA, operandB, ...operandN) -> operate()

Imagine a child just given an art kit, excited to explore their creativity, what's their first steps?

- Grab something
- Grab something else
- Put them together and see what happens


    (grab, grab) -> focus()

**_Focus?_**  

Well, yea!

Why would the child instinctively want to bring two things together?  Not to smash them to pieces - to make it
_cognitively easier_ to comprehend their shared existence!  Literally, _this_ is the act of mentally _focusing_
on a set of items.  The pathway, itself, to getting the items there is also a stack operation -

    grab:
        (moveArm, evaluateChoice) -> wait()

For every step of time, both movement and evaluation happen _concurrently_ as a cohesive _thought_.  The system
that sparked off the `grab` operation is aware of how to control the movement and evaluation functions, but without
_context_ `moveArm` and `evaluateChoice` have no target.