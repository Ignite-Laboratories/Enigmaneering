# `E0S1.0 - The Colonel of Kernels`
### `Alex Petz, Ignite Laboratories, March 2025`

---

### Does a neuron _execute_ or _activate?_

_Why not both?_   

The _entire concept_ of a neural impulse engine hinges on the concept of _activating_ a kernel of execution and 
letting it run _isolated_.  The key difference between a _neuron_ and a _kernel_ is that a neuron comes with a 
_potential function_ which must return `true` to begin execution.  While given a handle to control the neuron, 
the engine simply sparks off an asynchronous function and tracks if it should be re-invoked on every impulse.

Colloquially, the terms `activate` and `execute` are pretty interchangeable - but an _activation_ happens _cyclically!_
Neurons are microscopic little programmatic loops, at their core, constantly being stimulated with impulses from which
they perform the same calculations over and over again.  It's crucial to understand how activation happens across time.

<picture>
<img alt="Single Beat" src="assets/E0S1D0 - Single Beat.svg" width="250" style="display: block; margin-left: auto; margin-right: auto;">
</picture>

The first concept of activation is the _impulse_.  For every loop of the engine, an impulse of execution is coordinated.
Immediately, the current moment of _time_ is recorded and provided to every activation through a `std.Context`
structure.  The appropriate activations are invoked, the engine's current beat is incremented, and the process loops.

<picture>
<img alt="Multiple Beats" src="assets/E0S1D1 - Multiple Beats.svg" width="350" style="display: block; margin-left: auto; margin-right: auto;">
</picture>

Each cycle of the loop, itself, is an **impulse**.  Right now we are looking at the timeline of a loop currently at 
beat 42. Abstractly, there is no concept of _time_ through this lens - but the provided `std.Context` given by the engine
allows every activated kernel to know the **impulse moment** it's activation stemmed from.  When combined with
a beat value, kernels can intelligently decide how to respond.  Is it an even beat?  Odd?  Could it "skip"
every 15 beats by modulo-ing the current beat with 16?

These are the kinds of _potentials_ a impulse engine provides it's neurons! Let's look at a single
activation across time - this one is still executing and has yet to calculate a value:

<picture>
<img alt="Activation" src="assets/E0S1D2 - Activation.svg" width="400" style="display: block; margin-left: auto; margin-right: auto;">
</picture>

The first thing you can see is that an activation does _not_ begin at the impulse moment!  That's because coordinating
the launch of many activations requires _time_.  Above, that is chocked up purely to _entropy_ - but, thanks to the
`std.Context` every activation is temporally aware of _its_ relative offset to the impulse moment at all times.

The next thing you'll see is that an activation, when plotted across time as such, represents a _track_ of neural execution
on a _timeline_.  As more neurons are added to the system, they create a rich orchestration of intelligently looping
actions - kinda like a Digital Audio Workstation coordinating neural tracks of _intelligence_.

The final thing you should notice is that an activation _can_ span across multiple impulses - if it's _asynchronously_
invoked.  If not, it's considered a _blocking activation_ that quite literally slows the rate of impulse.

<picture>
<img alt="Blocking Activation" src="assets/E0S1D5 - Blocking Activation.svg" width="500" style="display: block; margin-left: auto; margin-right: auto;">
</picture>

Here, the activation finished and simply returned back the beat number it was passed.

As noted, this activation style blocks the next impulse from happening until it completes.  This quality can be leveraged 
to add _temporal weight_ to the system, slowing it's overall execution to ease load on the host architecture.  We don't 
need to worry about regulation _quite yet_, but it's still important to keep in mind!

If an activation is triggered _asynchronously_ it can be executed in one of several ways.  The first is an
*impulsive* activation which, as its name suggests, activates asynchronously on every impulse.

<picture>
<img alt="Impulsive Activation" src="assets/E0S1D4 - Impulsive Activation.svg" width="500" style="display: block; margin-left: auto; margin-right: auto;">
</picture>

As before, each activation just returns back the beat number from which it started. If it was instantiated
on beat 42, _that_ value gets associated with its starting grid position on the time axis - regardless of
the activation's overall execution time.

A more logical kind of activation is the _loop_.  In this configuration, it's cyclically invoked on the
next available impulse after completion.  The period between completion and re-invocation is the 
_refractory period_.

<picture>
<img alt="Cyclic Activation" src="assets/E0S1D3 - Cyclic Activation.svg" width="500" style="display: block; margin-left: auto; margin-right: auto;">
</picture>

Impulsive activations are useful for firing micro-calculations across time, while looping activations are
wonderful for performing more complex calculations that take longer.

### Clustered Activation

The next concept I'd like to explore is _clustered activation_.  It's quite simple - a wave of activations is provided
a synchronization pointer that blocks a "relay" activation used to re-stimulate the cluster.

It truly is that simple, but the concept is _powerful_ - we're still quite a way from utilizing this kind of activation,
but it's easy to understand at this point -

<picture>
<img alt="Clustered Activation" src="assets/E0S1D7 - Clustered Activation.svg" width="550" style="display: block; margin-left: auto; margin-right: auto;">
</picture>

### Temporal Fragment Shading

Before we proceed any further I want to briefly tease you with what this kind of activation scheme can yield.  Just as pixel
fragment shaders are executed in _parallel_ with their positional context, our activations are provided with their _temporal_ 
context.  While these activations occur _concurrently_, rather than necessarily in _parallel_, they can still refer to a 
specific index on a temporal frame buffer which is coalesced by a looping activation.  I don't intend to go too far into 
the weeds about that, yet - but visually it's easy to understand at this point in the process -

<picture>
<img alt="Temporal Fragment Shading" src="assets/E0S1D6 - Logical Activation.svg" width="500" style="display: block; margin-left: auto; margin-right: auto;">
</picture>

Each activation knows which _impulse index_ of the abstract timeline it should be "shading" in a value for.  A "coalesce" 
activation can observe the data recorded by these temporal fragment shaders at it's own pace and _its own activation window_ 
frames the range it should calculate next.  It knows which beat it started on, and the beat the last activation started 
on - thus, it can always reliably retrieve the exact indices since the last coalesce activation.  This creates a variable 
window of observable data _provided_ by the passing of _concurrent time_.

Because of this - the more impulses that can be recorded between coalescence, the more information each of the coalesce 
activations are provided.  This means that as calculations get more complex the rate of impulse should be _regulated_,
or else the system will continue to record at an unmanageable rate.  While the coalesce function _could_ be written
to ignore the extra data - why would you process stuff that'll never get utilized by your program's calculations?

    tl;dr - overall system load directly affects observational fidelity.